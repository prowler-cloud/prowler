---
title: 'Scan Results Import'
---

import { VersionBadge } from "/snippets/version-badge.mdx"

<VersionBadge version="5.14.0" />

**Prowler App** allows importing scan results from Prowler CLI executions, enabling centralized visibility of findings from distributed environments. This guide demonstrates how to import JSON/OCSF and CSV scan outputs through both the UI and API.

## Overview

The Scan Import feature enables security teams to:

- **Centralize findings** from isolated or air-gapped environments where Prowler CLI runs locally
- **Import historical scans** from existing Prowler CLI outputs
- **Aggregate results** from multiple distributed Prowler deployments into a single dashboard
- **Maintain compliance visibility** across environments with network restrictions

## Supported File Formats

Prowler App accepts two file formats for scan import:

### JSON/OCSF Format

The recommended format for importing scan results. OCSF (Open Cybersecurity Schema Framework) provides a standardized structure for security findings.

- File extension: `.json`
- Encoding: UTF-8
- Generated by Prowler CLI with default output settings
- Contains full finding metadata including compliance mappings

#### JSON Structure

The JSON file must be an array of finding objects. Each finding object follows the OCSF schema:

```json
[
  {
    "metadata": {
      "event_code": "check_id_here"
    },
    "finding_info": {
      "uid": "unique-finding-id",
      "title": "Check Title",
      "desc": "Check description"
    },
    "cloud": {
      "provider": "aws",
      "account": {
        "uid": "123456789012",
        "name": "Account Name"
      }
    },
    "severity": "high",
    "status_code": "FAIL",
    "status_detail": "Extended status message",
    "message": "Finding message",
    "risk_details": "Risk information",
    "remediation": {
      "desc": "How to fix this issue",
      "references": ["https://example.com/docs"]
    },
    "resources": [
      {
        "uid": "resource-arn-or-id",
        "name": "resource-name",
        "region": "us-east-1",
        "type": "AWS::S3::Bucket",
        "group": {
          "name": "s3"
        }
      }
    ],
    "unmapped": {
      "compliance": {
        "CIS-AWS-1.5": ["1.1", "1.2"],
        "PCI-DSS-4.0": ["2.1"]
      },
      "categories": ["security", "encryption"]
    },
    "time_dt": "2025-01-15T10:30:00.000000"
  }
]
```

#### Required JSON Fields

The following fields are required for each finding:

| Field Path | Description |
|------------|-------------|
| `metadata.event_code` | Check ID (e.g., `s3_bucket_public_access`) |
| `finding_info.uid` | Unique identifier for the finding |
| `cloud.provider` | Cloud provider type |
| `cloud.account.uid` | Cloud account identifier |

### CSV Format

Alternative format for importing scan results, useful for legacy exports or when JSON is not available.

- File extension: `.csv`
- Encoding: UTF-8
- Delimiter: Semicolon (`;`) by default, comma (`,`) also supported
- Includes all finding fields and compliance information

#### CSV Structure

The CSV file must include a header row with column names. Prowler CLI generates CSV files with semicolon delimiters by default.

```csv
FINDING_UID;PROVIDER;CHECK_ID;STATUS;ACCOUNT_UID;SEVERITY;RESOURCE_UID;RESOURCE_NAME;REGION;...
finding-001;aws;s3_bucket_public_access;FAIL;123456789012;high;arn:aws:s3:::my-bucket;my-bucket;us-east-1;...
```

#### Required CSV Columns

The following columns are required:

| Column | Description |
|--------|-------------|
| `FINDING_UID` | Unique identifier for the finding |
| `PROVIDER` | Cloud provider type |
| `CHECK_ID` | Check ID (e.g., `s3_bucket_public_access`) |
| `STATUS` | Finding status (`PASS`, `FAIL`, or `MANUAL`) |
| `ACCOUNT_UID` | Cloud account identifier |

#### All Supported CSV Columns

<Accordion title="Complete list of CSV columns">

| Column | Description |
|--------|-------------|
| `AUTH_METHOD` | Authentication method used |
| `TIMESTAMP` | Finding timestamp |
| `ACCOUNT_UID` | Cloud account identifier |
| `ACCOUNT_NAME` | Cloud account name |
| `ACCOUNT_EMAIL` | Account email |
| `ACCOUNT_ORGANIZATION_UID` | Organization identifier |
| `ACCOUNT_ORGANIZATION_NAME` | Organization name |
| `ACCOUNT_TAGS` | Account tags |
| `FINDING_UID` | Unique finding identifier |
| `PROVIDER` | Cloud provider type |
| `CHECK_ID` | Check identifier |
| `CHECK_TITLE` | Check title |
| `CHECK_TYPE` | Check type |
| `STATUS` | Finding status |
| `STATUS_EXTENDED` | Extended status message |
| `MUTED` | Whether finding is muted |
| `SERVICE_NAME` | Cloud service name |
| `SUBSERVICE_NAME` | Cloud subservice name |
| `SEVERITY` | Finding severity |
| `RESOURCE_TYPE` | Resource type |
| `RESOURCE_UID` | Resource identifier |
| `RESOURCE_NAME` | Resource name |
| `RESOURCE_DETAILS` | Resource details |
| `RESOURCE_TAGS` | Resource tags |
| `PARTITION` | Cloud partition |
| `REGION` | Cloud region |
| `DESCRIPTION` | Check description |
| `RISK` | Risk information |
| `RELATED_URL` | Related documentation URL |
| `REMEDIATION_RECOMMENDATION_TEXT` | Remediation guidance |
| `REMEDIATION_RECOMMENDATION_URL` | Remediation URL |
| `REMEDIATION_CODE_NATIVEIAC` | Native IaC remediation |
| `REMEDIATION_CODE_TERRAFORM` | Terraform remediation |
| `REMEDIATION_CODE_CLI` | CLI remediation |
| `REMEDIATION_CODE_OTHER` | Other remediation |
| `COMPLIANCE` | Compliance mappings |
| `CATEGORIES` | Finding categories |
| `DEPENDS_ON` | Dependencies |
| `RELATED_TO` | Related checks |
| `NOTES` | Additional notes |
| `PROWLER_VERSION` | Prowler version |
| `ADDITIONAL_URLS` | Additional URLs |

</Accordion>

#### Compliance Column Format

The `COMPLIANCE` column uses a pipe-separated format to list multiple compliance frameworks:

```
FRAMEWORK1: control1, control2 | FRAMEWORK2: control3, control4
```

Example:
```
CIS-AWS-1.5: 1.1, 1.2, 1.3 | PCI-DSS-4.0: 2.1.1 | SOC2: CC6.1
```

### Supported Values

#### Provider Types

Both formats support the following cloud provider types:

| Provider | Description |
|----------|-------------|
| `aws` | Amazon Web Services |
| `azure` | Microsoft Azure |
| `gcp` | Google Cloud Platform |
| `kubernetes` | Kubernetes clusters |
| `github` | GitHub repositories |
| `m365` | Microsoft 365 |
| `alibabacloud` | Alibaba Cloud |
| `nhn` | NHN Cloud |
| `oraclecloud` | Oracle Cloud |
| `mongodbatlas` | MongoDB Atlas |

#### Severity Levels

Valid severity values (case-insensitive):

- `critical`
- `high`
- `medium`
- `low`
- `informational`

#### Status Codes

Valid status values (case-insensitive):

- `PASS` - Check passed
- `FAIL` - Check failed
- `MANUAL` - Requires manual verification

<Note>
Both formats are automatically detected based on file content. The file extension is used as a hint but content structure takes precedence.
</Note>

## Importing via UI

### Step 1: Navigate to Scans

Access the Scans page from the main navigation menu.

### Step 2: Upload Scan File

1. Locate the **Import Scan** section at the top of the Scans page
2. Drag and drop your scan file (`.json` or `.csv`) into the upload area, or click to browse
3. The file name and size will be displayed after selection

<Warning>
**File Size Limit**

The maximum file size for scan imports is **50MB**. For larger scan outputs, consider splitting the results or using the API with streaming.
</Warning>

### Step 3: Configure Provider (Optional)

Before importing, you can optionally configure provider association:

- **Select existing provider**: Choose from the dropdown to associate findings with an existing cloud provider
- **Create provider automatically**: Leave the selection empty to let Prowler create or match a provider based on the scan data

<Note>
Provider information is extracted from the scan data automatically. If a matching provider (same account ID and provider type) already exists, findings will be associated with it.
</Note>

### Step 4: Import

1. Click the **Import** button to start the upload
2. A progress indicator shows the upload and processing status
3. Upon completion, you'll see:
   - Number of findings imported
   - Number of resources created
   - Link to view the imported scan

### Step 5: View Results

After successful import, click the scan link to view:

- All imported findings in the Findings view
- Compliance status across frameworks
- Resource inventory from the scan

## Importing via API

For automation and CI/CD integration, use the REST API endpoint directly.

### Endpoint

```
POST /api/v1/scans/import
```

### Authentication

Include your API key or JWT token in the request header:

```bash
Authorization: Api-Key <your-api-key>
# or
Authorization: Bearer <your-jwt-token>
```

### Request Formats

#### File Upload (Multipart)

```bash
curl -X POST \
  -H "Authorization: Api-Key ${PROWLER_API_KEY}" \
  -F "file=@prowler-output.json" \
  -F "provider_id=550e8400-e29b-41d4-a716-446655440000" \
  https://api.prowler.com/api/v1/scans/import
```

#### Inline JSON

```bash
curl -X POST \
  -H "Authorization: Api-Key ${PROWLER_API_KEY}" \
  -H "Content-Type: application/json" \
  -d @prowler-output.json \
  https://api.prowler.com/api/v1/scans/import
```

### Request Parameters

| Parameter | Type | Required | Description |
|-----------|------|----------|-------------|
| `file` | File | No* | Scan output file (JSON or CSV) |
| `data` | JSON | No* | Inline JSON data (alternative to file) |
| `provider_id` | UUID | No | Associate with existing provider |
| `create_provider` | Boolean | No | Create provider if not found (default: true) |

<Note>
*Either `file` or `data` must be provided, but not both.
</Note>

### Response

#### Success Response

```json
{
  "data": {
    "type": "scan-imports",
    "id": "550e8400-e29b-41d4-a716-446655440001",
    "attributes": {
      "scan_id": "550e8400-e29b-41d4-a716-446655440001",
      "findings_count": 1523,
      "resources_count": 245,
      "provider_id": "550e8400-e29b-41d4-a716-446655440000",
      "status": "completed"
    }
  }
}
```

#### Error Response

```json
{
  "errors": [
    {
      "status": "422",
      "code": "validation_error",
      "title": "Invalid OCSF format",
      "detail": "Missing required field 'metadata.event_code' at index 5",
      "source": {
        "pointer": "/data/findings/5/metadata/event_code"
      }
    }
  ]
}
```

## Field Mappings

This section provides detailed field mapping documentation for both JSON/OCSF and CSV formats, showing how imported data maps to Prowler's internal data models.

### JSON/OCSF to Prowler Models

The following tables show how OCSF fields map to Prowler's Finding, Resource, and Provider models.

#### Finding Fields

| OCSF Field | Prowler Model.Field | Description |
|------------|---------------------|-------------|
| `finding_info.uid` | `Finding.uid` | Unique identifier for the finding |
| `metadata.event_code` | `Finding.check_id` | Check identifier (e.g., `s3_bucket_public_access`) |
| `status_code` | `Finding.status` | Finding status (`PASS`, `FAIL`, `MANUAL`) |
| `status_detail` | `Finding.status_extended` | Extended status message with details |
| `severity` | `Finding.severity` | Severity level (`critical`, `high`, `medium`, `low`, `informational`) |
| `severity` | `Finding.impact` | Impact level (same as severity) |
| `message` | `Finding.impact_extended` | Extended impact description |
| `time_dt` | `Finding.first_seen_at` | Timestamp when finding was first detected |
| `unmapped.compliance` | `Finding.compliance` | Compliance framework mappings |
| *(raw data)* | `Finding.raw_result` | Complete original OCSF finding object |

#### Check Metadata Fields

| OCSF Field | Prowler Model.Field | Description |
|------------|---------------------|-------------|
| `finding_info.title` | `Finding.check_metadata.title` | Human-readable check title |
| `finding_info.desc` | `Finding.check_metadata.description` | Detailed check description |
| `risk_details` | `Finding.check_metadata.risk` | Risk information and business impact |
| `remediation.desc` | `Finding.check_metadata.remediation.description` | How to remediate the finding |
| `remediation.references` | `Finding.check_metadata.remediation.references` | URLs to remediation documentation |
| `unmapped.categories` | `Finding.check_metadata.categories` | Finding categories (e.g., `security`, `encryption`) |
| `unmapped.related_url` | `Finding.check_metadata.related_url` | Related documentation URL |

#### Resource Fields

| OCSF Field | Prowler Model.Field | Description |
|------------|---------------------|-------------|
| `resources[].uid` | `Resource.uid` | Unique resource identifier (ARN, ID, etc.) |
| `resources[].name` | `Resource.name` | Human-readable resource name |
| `resources[].region` | `Resource.region` | Cloud region where resource is located |
| `resources[].group.name` | `Resource.service` | Cloud service name (e.g., `s3`, `ec2`) |
| `resources[].type` | `Resource.type` | Resource type (e.g., `AWS::S3::Bucket`) |

#### Provider Fields

| OCSF Field | Prowler Model.Field | Description |
|------------|---------------------|-------------|
| `cloud.provider` | `Provider.provider` | Cloud provider type (`aws`, `azure`, `gcp`, etc.) |
| `cloud.account.uid` | `Provider.uid` | Cloud account identifier |
| `cloud.account.name` | `Provider.alias` | Account name/alias (optional) |

### CSV to Prowler Models

The following tables show how CSV columns map to Prowler's Finding, Resource, and Provider models.

#### Finding Fields

| CSV Column | Prowler Model.Field | Description |
|------------|---------------------|-------------|
| `FINDING_UID` | `Finding.uid` | Unique identifier for the finding |
| `CHECK_ID` | `Finding.check_id` | Check identifier (e.g., `s3_bucket_public_access`) |
| `STATUS` | `Finding.status` | Finding status (`PASS`, `FAIL`, `MANUAL`) |
| `STATUS_EXTENDED` | `Finding.status_extended` | Extended status message with details |
| `SEVERITY` | `Finding.severity` | Severity level (`critical`, `high`, `medium`, `low`, `informational`) |
| `SEVERITY` | `Finding.impact` | Impact level (same as severity) |
| `STATUS_EXTENDED` | `Finding.impact_extended` | Extended impact description |
| `TIMESTAMP` | `Finding.first_seen_at` | Timestamp when finding was first detected |
| `COMPLIANCE` | `Finding.compliance` | Compliance framework mappings (parsed from pipe-separated format) |
| `MUTED` | `Finding.muted` | Whether finding is muted (`true`/`false`) |
| *(raw row)* | `Finding.raw_result` | Complete original CSV row as dictionary |

#### Check Metadata Fields

| CSV Column | Prowler Model.Field | Description |
|------------|---------------------|-------------|
| `CHECK_TITLE` | `Finding.check_metadata.title` | Human-readable check title |
| `DESCRIPTION` | `Finding.check_metadata.description` | Detailed check description |
| `RISK` | `Finding.check_metadata.risk` | Risk information and business impact |
| `REMEDIATION_RECOMMENDATION_TEXT` | `Finding.check_metadata.remediation.description` | How to remediate the finding |
| `REMEDIATION_RECOMMENDATION_URL` | `Finding.check_metadata.remediation.url` | URL to remediation documentation |
| `REMEDIATION_CODE_CLI` | `Finding.check_metadata.remediation.cli` | CLI command for remediation |
| `REMEDIATION_CODE_TERRAFORM` | `Finding.check_metadata.remediation.terraform` | Terraform code for remediation |
| `REMEDIATION_CODE_NATIVEIAC` | `Finding.check_metadata.remediation.nativeiac` | Native IaC code for remediation |
| `REMEDIATION_CODE_OTHER` | `Finding.check_metadata.remediation.other` | Other remediation code |
| `CATEGORIES` | `Finding.check_metadata.categories` | Finding categories (comma-separated) |
| `RELATED_URL` | `Finding.check_metadata.related_url` | Related documentation URL |
| `ADDITIONAL_URLS` | `Finding.check_metadata.additional_urls` | Additional URLs (pipe-separated) |
| `NOTES` | `Finding.check_metadata.notes` | Additional notes |

#### Resource Fields

| CSV Column | Prowler Model.Field | Description |
|------------|---------------------|-------------|
| `RESOURCE_UID` | `Resource.uid` | Unique resource identifier (ARN, ID, etc.) |
| `RESOURCE_NAME` | `Resource.name` | Human-readable resource name |
| `REGION` | `Resource.region` | Cloud region where resource is located |
| `SERVICE_NAME` | `Resource.service` | Cloud service name (e.g., `s3`, `ec2`) |
| `RESOURCE_TYPE` | `Resource.type` | Resource type (e.g., `AWS::S3::Bucket`) |
| `PARTITION` | `Resource.partition` | Cloud partition (e.g., `aws`, `aws-gov`) |
| `RESOURCE_TAGS` | `Resource.tags` | Resource tags |
| `RESOURCE_DETAILS` | `Resource.details` | Additional resource details |

#### Provider Fields

| CSV Column | Prowler Model.Field | Description |
|------------|---------------------|-------------|
| `PROVIDER` | `Provider.provider` | Cloud provider type (`aws`, `azure`, `gcp`, etc.) |
| `ACCOUNT_UID` | `Provider.uid` | Cloud account identifier |
| `ACCOUNT_NAME` | `Provider.alias` | Account name/alias (optional) |

### Scan Record Fields

When importing scan results, Prowler creates a Scan record with the following field mappings:

| Source | Prowler Model.Field | Description |
|--------|---------------------|-------------|
| *(constant)* | `Scan.trigger` | Set to `imported` to identify imported scans |
| *(constant)* | `Scan.state` | Set to `completed` |
| *(min timestamp)* | `Scan.started_at` | Earliest finding timestamp in the import |
| *(max timestamp)* | `Scan.completed_at` | Latest finding timestamp in the import |
| *(calculated)* | `Scan.duration` | Difference between started_at and completed_at |
| *(calculated)* | `Scan.unique_resource_count` | Count of unique resources in the import |
| *(constant)* | `Scan.progress` | Set to `100` (complete) |

### Data Normalization

During import, certain fields are normalized for consistency:

| Field | Normalization |
|-------|---------------|
| `severity` | Converted to lowercase (`critical`, `high`, `medium`, `low`, `informational`) |
| `status` | Converted to uppercase (`PASS`, `FAIL`, `MANUAL`) |
| `provider` | Converted to lowercase (`aws`, `azure`, `gcp`, etc.) |
| `compliance` | Parsed from pipe-separated format (CSV) or kept as object (JSON) |
| `timestamp` | Parsed from ISO 8601 or common date formats |

### Default Values

When optional fields are missing, the following defaults are applied:

| Field | Default Value |
|-------|---------------|
| `severity` | `informational` |
| `status` | `MANUAL` |
| `resource.name` | Uses `resource.uid` if name is empty |
| `timestamp` | Current UTC time if not provided |
| `muted` | `false` |

## Provider Association

When importing scan results, Prowler handles provider association automatically:

1. **Extract provider info**: Account ID and provider type are extracted from the scan data
2. **Match existing provider**: If a provider with the same account ID and type exists, findings are associated with it
3. **Create new provider**: If no match is found and `create_provider` is enabled, a new provider is created
4. **Manual selection**: Optionally specify `provider_id` to force association with a specific provider

<Note>
Imported scans are marked with trigger type **"imported"** to distinguish them from scheduled or manual scans initiated through Prowler App.
</Note>

## Use Cases

### Air-Gapped Environments

For environments without internet access:

1. Run Prowler CLI locally: `prowler aws --output-formats json`
2. Transfer the output file to a system with Prowler App access
3. Import via UI or API

### CI/CD Pipeline Integration

Integrate scan imports into your deployment pipeline:

```yaml
# Example GitHub Actions step
- name: Import Prowler Results
  run: |
    curl -X POST \
      -H "Authorization: Api-Key ${{ secrets.PROWLER_API_KEY }}" \
      -F "file=@prowler-output.json" \
      https://api.prowler.com/api/v1/scans/import
```

### Multi-Account Aggregation

Aggregate results from multiple AWS accounts:

1. Run Prowler CLI in each account
2. Collect output files centrally
3. Import each file to Prowler App
4. View consolidated findings across all accounts

## Troubleshooting

This section provides detailed guidance for resolving common issues when importing scan results.

### Format Detection Errors

#### "File format not recognized"

This error occurs when the import service cannot identify the file as valid JSON/OCSF or CSV.

**Possible causes:**
- File is corrupted or truncated
- File uses an unsupported encoding (must be UTF-8)
- File is not a Prowler output file

**Solutions:**
1. Verify the file opens correctly in a text editor
2. Check the file encoding is UTF-8:
   ```bash
   file -i your-scan-file.json
   # Should show: charset=utf-8
   ```
3. Regenerate the scan output from Prowler CLI
4. If using CSV, ensure the file has proper headers

#### "Invalid JSON" or "Invalid JSON syntax"

**Possible causes:**
- Malformed JSON structure
- File was truncated during transfer
- Special characters causing parsing issues

**Solutions:**
1. Validate JSON syntax using a tool like `jq`:
   ```bash
   jq . your-scan-file.json > /dev/null
   # If valid, no output; if invalid, shows error location
   ```
2. Check for truncation by verifying the file ends with `]`
3. Look for encoding issues with special characters

#### "Expected JSON array of findings"

The JSON file must contain an array `[...]` at the root level, not a single object `{...}`.

**Solution:**
Ensure your Prowler CLI output is the findings array, not a wrapper object. The correct format is:
```json
[
  { "metadata": {...}, "finding_info": {...}, ... },
  { "metadata": {...}, "finding_info": {...}, ... }
]
```

### JSON/OCSF Validation Errors

#### "Missing required field 'metadata.event_code'"

Each finding must have a check ID in the `metadata.event_code` field.

**Solution:**
Verify your OCSF output includes the metadata section:
```json
{
  "metadata": {
    "event_code": "s3_bucket_public_access"
  }
}
```

#### "Missing required field 'finding_info.uid'"

Each finding must have a unique identifier.

**Solution:**
Ensure the `finding_info.uid` field is present and non-empty in each finding.

#### "Missing required field 'cloud.provider'"

Provider type is required to associate findings with the correct cloud account.

**Solution:**
Verify the `cloud.provider` field exists with a valid value (`aws`, `azure`, `gcp`, etc.).

#### "Missing required field 'cloud.account.uid'"

Account identifier is required for provider association.

**Solution:**
Ensure `cloud.account.uid` contains the cloud account ID (e.g., AWS account number, Azure subscription ID).

#### "Unknown provider type"

The provider type in the scan data is not recognized.

**Supported provider types:**
- `aws` - Amazon Web Services
- `azure` - Microsoft Azure
- `gcp` - Google Cloud Platform
- `kubernetes` - Kubernetes clusters
- `github` - GitHub repositories
- `m365` - Microsoft 365
- `alibabacloud` - Alibaba Cloud
- `nhn` - NHN Cloud
- `oraclecloud` - Oracle Cloud
- `mongodbatlas` - MongoDB Atlas

**Solution:**
Check the `cloud.provider` value matches one of the supported types (case-insensitive).

### CSV Validation Errors

#### "Missing required CSV columns"

The CSV file is missing one or more required columns.

**Required columns:**
- `FINDING_UID` - Unique finding identifier
- `PROVIDER` - Cloud provider type
- `CHECK_ID` - Check identifier
- `STATUS` - Finding status (PASS, FAIL, MANUAL)
- `ACCOUNT_UID` - Cloud account identifier

**Solution:**
1. Verify the CSV was generated by Prowler CLI
2. Check that the header row is present and not corrupted
3. Ensure column names match exactly (case-sensitive)

#### "CSV has no headers"

The CSV file is missing the header row.

**Solution:**
Ensure the first line of the CSV contains column headers:
```csv
FINDING_UID;PROVIDER;CHECK_ID;STATUS;ACCOUNT_UID;...
```

#### "Missing required value for column"

A required field is empty in one or more rows.

**Solution:**
1. Check the error message for the specific row number and column
2. Open the CSV and verify the row has values for all required columns
3. Look for rows with missing data or incorrect delimiter usage

#### "CSV parsing error"

General CSV parsing failure, often due to malformed data.

**Possible causes:**
- Incorrect delimiter (Prowler uses semicolon `;` by default)
- Unescaped special characters in field values
- Inconsistent number of columns across rows

**Solutions:**
1. Verify the delimiter matches your file (semicolon or comma)
2. Check for fields containing the delimiter character without proper quoting
3. Ensure all rows have the same number of columns as the header

### Provider Resolution Errors

#### "Provider with ID not found"

The specified `provider_id` does not exist or belongs to a different tenant.

**Solutions:**
1. Verify the provider ID is correct
2. Check that the provider exists in your Prowler App instance
3. Ensure you're authenticated with the correct tenant

#### "No provider found for account"

No existing provider matches the scan data, and `create_provider` is disabled.

**Solutions:**
1. Set `create_provider` to `true` to automatically create the provider
2. Manually create the provider in Prowler App before importing
3. Specify an existing `provider_id` to associate with

#### "Unsupported provider type"

The provider type extracted from scan data is not supported.

**Solution:**
Verify the provider type in your scan data matches one of the supported types listed above.

### File Size and Performance Issues

#### "File size exceeds maximum"

The import file exceeds the 50MB limit.

**Solutions:**
1. Split large scan outputs into smaller files by filtering:
   ```bash
   # Split by status
   jq '[.[] | select(.status_code == "FAIL")]' large-scan.json > failed-findings.json
   ```
2. Import findings in batches by region or service
3. Consider running more targeted scans with fewer checks

#### Import is slow or times out

Large imports with many findings may take time to process.

**Expected performance:**
- 10,000 findings: ~60 seconds
- Bulk operations are used for efficiency

**Solutions:**
1. For very large imports, split into multiple smaller files
2. Import during off-peak hours
3. Check server resources if self-hosting

### Authentication and Permission Errors

#### "Authentication required" (401)

The request is missing authentication credentials.

**Solutions:**
1. Include the `Authorization` header with your API key or JWT:
   ```bash
   -H "Authorization: Api-Key YOUR_API_KEY"
   # or
   -H "Authorization: Bearer YOUR_JWT_TOKEN"
   ```
2. Verify your credentials are valid and not expired

#### "Permission denied" (403)

Your user account lacks the required permission.

**Solution:**
The `MANAGE_SCANS` permission is required for scan imports. Contact your administrator to grant this permission.

#### "Token expired"

JWT tokens have a limited lifetime.

**Solution:**
Obtain a fresh JWT token and retry the request.

### Data Integrity Issues

#### "No findings found in the imported file"

The file was parsed successfully but contains no findings.

**Solutions:**
1. Verify the scan actually produced findings
2. Check if findings were filtered out during export
3. Ensure the file is not just headers (CSV) or an empty array (JSON)

#### Duplicate findings

Importing the same scan multiple times creates duplicate findings.

**Current behavior:**
- Each import creates a new Scan record
- Findings are created fresh for each import
- Resources are deduplicated (existing resources are reused)

**Solutions:**
1. Track which scans have been imported to avoid duplicates
2. Use unique scan identifiers to prevent re-importing

#### Missing compliance mappings

Compliance information is not appearing for imported findings.

**Solutions:**
1. **JSON/OCSF**: Verify `unmapped.compliance` contains the framework mappings:
   ```json
   "unmapped": {
     "compliance": {
       "CIS-AWS-1.5": ["1.1", "1.2"]
     }
   }
   ```
2. **CSV**: Check the `COMPLIANCE` column uses the correct format:
   ```
   CIS-AWS-1.5: 1.1, 1.2 | PCI-DSS-4.0: 2.1
   ```

### Common Error Codes Reference

| HTTP Status | Error Code | Description |
|-------------|------------|-------------|
| 400 | `invalid_format` | File format not recognized as JSON or CSV |
| 400 | `json_parse_error` | Invalid JSON syntax or structure |
| 400 | `csv_parse_error` | Invalid CSV syntax or structure |
| 400 | `no_findings` | File contains no findings to import |
| 400 | `invalid_provider_id` | Provider ID format is invalid |
| 404 | `provider_not_found` | Specified provider does not exist |
| 413 | `file_too_large` | File exceeds 50MB size limit |
| 422 | `validation_error` | Data validation failed (see details) |
| 422 | `invalid_provider_type` | Provider type not supported |

### Getting Help

If you continue to experience issues:

1. **Check the error details**: API responses include detailed error information with field paths and row numbers
2. **Validate your file**: Use the validation tips above to verify file format
3. **Review the logs**: Server logs contain additional debugging information
4. **Contact support**: Provide the error message, file format, and a sanitized sample of your data

## Required Permissions

Importing scans requires the **MANAGE_SCANS** RBAC permission. Users without this permission will not see the import option in the UI and API requests will be rejected.

For more information about RBAC permissions, refer to the [Prowler App RBAC documentation](/user-guide/tutorials/prowler-app-rbac).
