---
title: 'Using Multiple LLM Providers with Lighthouse'
---

import { VersionBadge } from "/snippets/version-badge.mdx"

<VersionBadge version="5.14.0" />

Prowler Lighthouse AI supports multiple Large Language Model (LLM) providers, offering flexibility to choose the provider that best fits infrastructure, compliance requirements, and cost considerations. This guide explains how to configure and use different LLM providers with Lighthouse AI.

## Supported Providers

Lighthouse AI supports the following LLM providers:

- **OpenAI**: Provides access to GPT models (GPT-4o, GPT-4, etc.)
- **Amazon Bedrock**: Offers AWS-hosted access to Claude, Llama, Titan, and other models
- **OpenAI Compatible**: Supports custom endpoints like OpenRouter, Ollama, or any OpenAI-compatible service

## How Default Providers Work

All three providers can be configured for a tenant, but only one can be set as the default provider. The first configured provider automatically becomes the default.

When visiting Lighthouse AI chat, the default provider's default model loads automatically. Users can switch to any available LLM model (including those from non-default providers) using the dropdown in chat.

<img src="/images/prowler-app/lighthouse-switch-models.png" alt="Switch models in Lighthouse AI chat interface" />

## Configuring Providers

Navigate to **Configuration** → **Lighthouse AI** to see all three provider options with a **Connect** button under each.

<img src="/images/prowler-app/lighthouse-configuration.png" alt="Prowler Lighthouse Configuration" />

### Connecting a Provider

To connect a provider:

1. Click **Connect** under the desired provider
2. Enter the required credentials
3. Select a default model for that provider
4. Click **Connect** to save

### OpenAI

#### Required Information

- **API Key**: OpenAI API key (starts with `sk-` or `sk-proj-`)

<Note>
To generate an OpenAI API key, visit https://platform.openai.com/api-keys
</Note>

### Amazon Bedrock

Prowler connects to Amazon Bedrock using either [Amazon Bedrock API keys](https://docs.aws.amazon.com/bedrock/latest/userguide/getting-started-api-keys.html) or IAM credentials.

<Note>
Amazon Bedrock models depend on AWS region and account entitlements. Lighthouse AI displays only accessible models that support tool calling and text input/output.
</Note>

#### Amazon Bedrock API Key

<Warning>
Recommended only for exploration of Amazon Bedrock.
</Warning>

Amazon Bedrock API keys provide simpler authentication with automatically assigned permissions.

##### Required Information

- **Bedrock API Key**: Amazon Bedrock API key
- **AWS Region**: Region where Bedrock is available (e.g., `us-east-1`, `us-west-2`)

<Note>
Amazon Bedrock API keys are automatically assigned the necessary permissions (`AmazonBedrockLimitedAccess` policy).

Learn more: [Getting Started with Amazon Bedrock API Keys](https://docs.aws.amazon.com/bedrock/latest/userguide/getting-started-api-keys.html)
</Note>

#### AWS IAM Access Keys

Standard AWS IAM credentials can be used as an alternative authentication method.

##### Required Information

- **AWS Access Key ID**: AWS access key ID
- **AWS Secret Access Key**: AWS secret access key
- **AWS Region**: Region where Bedrock is available (e.g., `us-east-1`, `us-west-2`)

##### Required Permissions

The AWS IAM user must have the `AmazonBedrockLimitedAccess` managed policy attached:

```text
arn:aws:iam::aws:policy/AmazonBedrockLimitedAccess
```

### OpenAI Compatible

Use this option to connect to any LLM provider exposing OpenAI compatible API endpoint (OpenRouter, Ollama, etc.).

#### Required Information

- **API Key**: API key from the compatible service
- **Base URL**: API endpoint URL including the API version (e.g., `https://openrouter.ai/api/v1`)

#### Example: OpenRouter

1. Create an account at [OpenRouter](https://openrouter.ai/)
2. [Generate an API key](https://openrouter.ai/docs/guides/overview/auth/provisioning-api-keys) from the OpenRouter dashboard
3. Configure in Lighthouse AI:
   - **API Key**: OpenRouter API key
   - **Base URL**: `https://openrouter.ai/api/v1`

## Changing the Default Provider

To set a different provider as default:

1. Navigate to **Configuration** → **Lighthouse AI**
2. Click **Configure** under the provider you want as default
3. Click **Set as Default**

<img src="/images/prowler-app/lighthouse-set-default-provider.png" alt="Set default LLM provider" />

## Updating Provider Credentials

To update credentials for a connected provider:

1. Navigate to **Configuration** → **Lighthouse AI**
2. Click **Configure** under the provider
3. Enter the new credentials
4. Click **Update**

## Deleting a Provider

To remove a configured provider:

1. Navigate to **Configuration** → **Lighthouse AI**
2. Click **Configure** under the provider
3. Click **Delete**

## Model Recommendations

For best results with Lighthouse AI, the recommended model is `gpt-5` from OpenAI.

Models from other providers such as Amazon Bedrock and OpenAI Compatible endpoints can be connected and used, but performance is not guaranteed.

## Getting Help

For issues or suggestions, [reach out through our Slack channel](https://goto.prowler.com/slack).
